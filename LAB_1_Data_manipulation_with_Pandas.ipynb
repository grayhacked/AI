{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grayhacked/AI/blob/main/LAB_1_Data_manipulation_with_Pandas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNHkwnbO_uuR"
      },
      "source": [
        "# LAB1. CNN manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IKdEqQ82_uuZ"
      },
      "outputs": [],
      "source": [
        "#Importation des bibliotheques\n",
        "import torch  #Bibliotheques pour le deep learning\n",
        "import torch.nn as nn  #Bibliotheques pour creer les reseaux de neurones\n",
        "import torch.optim as optim  #Bibliotheques pour l'optimisation du modele\n",
        "from torchvision import datasets, transforms  #Bibliotheques pour charger les donnees d'images comme MNIST\n",
        "from torch.utils.data import DataLoader  #Bibliotheques pour creer des dataloaders,  Pour gérer les lots (batches) de données\n",
        "#: DataLoader Permet de charger les données en petits morceaux appelés batches, pour les traiter plus facilement.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Étape 2 : Chargement des Données\n",
        "\n",
        "# Transformation des images (normalisation et conversion en tenseur)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),# Convertir une image en tenseur (tableau numérique manipulable par PyTorch)\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalisation pour une moyenne de 0 et un écart-type de 1, Normaliser les valeurs des pixels (les ramener entre -1 et 1)\n",
        "])\n",
        "\n",
        "# Chargement des datasets MNIST\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True) #Ensemble d'entraînement (images utilisées pour entraîner le modèle).\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True) #Ensemble de test (images utilisées pour évaluer le modèle après l'entraînement).\n",
        "\n",
        "# DataLoader pour itérer sur les datasets\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "'''\n",
        "DataLoader :\n",
        "\n",
        "Coupe les données en petits groupes (batches) de 64 images pour un traitement plus rapide.\n",
        "shuffle=True mélange les données à chaque époque pour que le modèle ne \"triche\" pas en apprenant l'ordre des images.\n",
        "'''\n"
      ],
      "metadata": {
        "id": "VjGK40SYKQXH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Étape 3 : Définition d’un CNN Simple\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)  # 1 canal (gris) -> 16 filtres\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # Réduction de la taille\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)  # 16 -> 32 filtres\n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 128)  # FC1 (entrée : 32 filtres réduits à 7x7)\n",
        "        self.fc2 = nn.Linear(128, 10)  # FC2 (10 classes pour MNIST)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))  # Convolution + ReLU + Pooling\n",
        "        x = self.pool(torch.relu(self.conv2(x)))  # Deuxième couche\n",
        "        x = x.view(-1, 32 * 7 * 7)  # Aplatir les tenseurs\n",
        "        x = torch.relu(self.fc1(x))  # Couche Fully Connected 1\n",
        "        x = self.fc2(x)  # Couche Fully Connected 2\n",
        "        return x\n",
        "\n",
        "'''\n",
        "Explications ligne par ligne :\n",
        "Couches convolutives :\n",
        "\n",
        "nn.Conv2d(1, 16, kernel_size=3) : La première couche prend une image avec 1 canal (niveau de gris) et produit 16 cartes de caractéristiques.\n",
        "nn.Conv2d(16, 32, kernel_size=3) : La deuxième couche prend les 16 cartes produites et en crée 32.\n",
        "ReLU :\n",
        "\n",
        "nn.ReLU() : Applique la fonction d'activation ReLU après chaque convolution.\n",
        "\n",
        "MaxPooling :\n",
        "nn.MaxPool2d(kernel_size=2, stride=2) : Réduit la taille des cartes de caractéristiques de moitié.\n",
        "\n",
        "Fully Connected Layers :\n",
        "nn.Linear(32 * 7 * 7, 128) : Transforme les cartes de 32x7x7 en un vecteur de taille 128.\n",
        "nn.Linear(128, 10) : Sortie de taille 10 (une pour chaque chiffre de 0 à 9).\n",
        "\n",
        "Fonction forward :\n",
        "Spécifie le chemin que les données suivent dans le réseau.\n",
        "Applique ReLU après chaque convolution et utilise MaxPooling pour réduire la taille.\n",
        "'''"
      ],
      "metadata": {
        "id": "hPB5P9vVK4ux"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Étape 4 : Entraînement du Modèle\n",
        "# Initialisation du modèle, de l’optimiseur et de la fonction de perte\n",
        "\n",
        "model = SimpleCNN() # On crée une instance de notre modèle\n",
        "criterion = nn.CrossEntropyLoss() # Fonction de perte pour la classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Optimiseur Adam pour ajuster les poids du modèle\n",
        "\n",
        "# Boucle d'entraînement\n",
        "for epoch in range(5):  # Entraîner pendant 5 époques\n",
        "    model.train() # Mode entraînement\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad() # Réinitialise les gradients\n",
        "        outputs = model(images)  # Passe avant (forward)\n",
        "        loss = criterion(outputs, labels) # Calcule la perte\n",
        "        loss.backward() # Calcul des gradients (backpropagation)\n",
        "        optimizer.step() # Mise à jour des poids\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Époque {epoch+1}, Perte : {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "\n",
        "'''\n",
        "Explications :\n",
        "Critère : La fonction de perte mesure l’écart entre les prédictions et les vraies étiquettes.\n",
        "Optimiseur : Ajuste les poids pour réduire la perte.\n",
        "Boucle d’entraînement :\n",
        "optimizer.zero_grad() : Réinitialise les gradients pour éviter leur accumulation.\n",
        "loss.backward() : Propagation des erreurs pour ajuster les poids.\n",
        "optimizer.step() : Mise à jour des poids.\n",
        "'''"
      ],
      "metadata": {
        "id": "SIjLORbYLlc2",
        "outputId": "b6f3cf7e-052a-4642-b2f1-3297554660a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Époque 1, Perte : 0.1960\n",
            "Époque 2, Perte : 0.0527\n",
            "Époque 3, Perte : 0.0374\n",
            "Époque 4, Perte : 0.0291\n",
            "Époque 5, Perte : 0.0219\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Étape 5 : Évaluation du Modèle\n",
        "# Test du modèle sur les données de test\n",
        "\n",
        "model.eval() # # Mode évaluation (désactive certaines fonctions comme le dropout)\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():  # Pas de calcul de gradients ici\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)  # Prédictions\n",
        "        _, predicted = torch.max(outputs.data, 1)  # Choix de la classe avec la probabilité la plus élevée\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Exactitude sur le dataset de test : {100 * correct / total:.2f}%\")\n",
        "\n",
        "'''\n",
        "Explications :\n",
        "Mode évaluation : Empêche le recalcul des gradients.\n",
        "Prédictions :\n",
        "torch.max(outputs.data, 1) : Retourne la classe ayant la probabilité maximale.\n",
        "Calcul de l'exactitude :\n",
        "Compare les prédictions aux vraies étiquettes.\n",
        "'''"
      ],
      "metadata": {
        "id": "f4nE7_UZM9my",
        "outputId": "526aed88-c957-4243-8a52-f32d6d3f372d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exactitude sur le dataset de test : 98.93%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "reqGJqrjOEyy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "a621c37eb283722c3e7ab138ba8d18d8251f90a9b461d6214c10aef376695899"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}